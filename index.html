<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Algebraic Methods in Optimization</title>

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="reveal.css">
		<link rel="stylesheet" href="solarized.css" id="theme">
        <style>
            p {font-size: 30px}
            li {font-size: 30px}
        </style>
	</head>

	<body>

		<div class="reveal">

			<div class="slides">

				<section>
					<h2>Algebraic Methods in Optimization</h2>
					<p>Kevin Shu</p>
				</section>

                <section>
					<h3 style="margin-top:100px">Topics</h3>
                    <section>
                    <p>
                    <ul style="font-size:27px">
                        <li>Accelerating Gradient Descent</li>
                        <li>Hidden Convexity and Homotopy Theory</li>
                        <li>Hyperbolic Polynomials and Eigenvalue Problems</li>
                    </ul>
                    </p>
                    </section>
                </section>

                <section>
                    <section>
                        <h3>Accelerated Gradient Descent</h3>
                    </section>
                    <section>
                        <h3> Based on joint work with Ben Grimmer and Alex Wang </h3>
                    </section>
                    <section data-auto-animate>
                        <h4>Accelerated Gradient Descent</h4>
                        <img src="hill1.png"> </img>
                    </section>
                    <section data-auto-animate>
                        <h4>Accelerated Gradient Descent</h4>
                        <img src="arrowHill.png"> </img>
                    </section>
                    <section data-auto-animate>
                        <h4>Accelerated Gradient Descent</h4>
                        <img src="onestep.png"> </img>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Accelerated Gradient Descent</h3>
                    </section>
                    <section>
                        <h3>Accelerated Gradient Descent</h3>
                        <p>
                            Given a smooth convex function \(f\), we want to find the minimum of \(f\) using first order queries.
                        </p>
                        <ul class="fragment fade-in">
                            <li><b> Convex</b> - $f(t x + (1-t) y) \le tf(x) + (1-t)f(y)$ whenever $t \in [0,1]$.</li>
                            <li><b>Smooth</b> - $f$ is continuously differentiable, and its derivative is $L$-lipschitz. For now, set $L = 1$.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Accelerated Gradient Descent</h3>
                        <p>
                            The classic way to do this is to use gradient descent: starting at some point $x_0$, we iterate
                            \[
                                x_{k+1} = x_k - \eta_k \nabla f(x_k),
                            \]
                        </p>
                        <ul>
                        <li class="fragment fade-in"><p>
                            Using $\eta_k = 1$ for each $k$ leads to a convergence rate of $O(1/T)$.
                        </p></li>
                        <li class="fragment fade-in"><p>
                            The best asymptotic rate comes from Nesterov acceleration, which has a convergence rate of \(O(1/T^2)\), but does not have this gradient descent form.
                        </p></li>
                    </ul>
                    </section>
                    <section>
                        <div style="background: grey; color: white; padding: 20px 20px 20px 20px">
                            <h5 style="color: white;">Question</h5>
                            <p>
                                Can we do better than $O(1/T)$ just by changing the step sizes in gradient descent?
                            </p>
                        </div>
                        <p class="fragment fade-in">
                            <b>Yes!</b>
                        </p>
                    </section>
                    <section>
                        <h4>Why can we do better than $O(1/T)$?</h4>
                        <div style="display:flex">
                            <div style="flex:1">
                                <img src="steep_hill.png"></img>
                                <p> <b>Steep hill</b> </p>
                                <p> Big steps - Divergence</p>
                            </div>
                            <div style="flex:1">
                                <img src="shallow_hill.png"></img>
                                <p> <b>Shallow hill</b> </p>
                                <p> Short steps - Slow convergence</p>
                            </div>
                        </div>
                        <p class="fragment fade-in">
                            <b>Alternating between big steps and long steps leads to faster convergence.</b>
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4>Our step sizes</h4>
                        <div style="display:flex">
                            <div style="flex:1; background: grey; color: white; padding: 20px 20px 20px 20px; margin-right:10px">
                                <h5 style="color: white;">Theorem (Grimmer, S., Wang)</h5>
                                <p>
                                    There is a sequence of step sizes achieving a rate of convergence of $O(1/T^{1.01})$.
                                </p>
                            </div>
                            <div style="flex:1; background: grey; color: white; padding: 20px 20px 20px 20px" class="fragment fade-in">
                                <h5 style="color: white;">Theorem (Altschuler, Parrilo)</h5>
                                <p>
                                    There is a sequence of step sizes achieving a rate of convergence of $O(1/T^{1.27})$.
                                </p>
                            </div>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4>Our step sizes</h4>
                        <div style="display:flex">
                            <div style="flex:1; background: grey; color: white; padding: 20px 20px 20px 20px; margin-right:10px">
                                <h5 style="color: white;">Theorem (Grimmer, S., Wang)</h5>
                                <p style="color: #F2F2FF">
                                    There is a sequence of step sizes achieving a rate of convergence of $O(1/T^{1.27})$ (with slightly better constants).
                                </p>
                            </div>
                            <div style="flex:1; background: grey; color: white; padding: 20px 20px 20px 20px">
                                <h5 style="color: white;">Theorem (Altschuler, Parrilo)</h5>
                                <p>
                                    There is a sequence of step sizes achieving a rate of convergence of $O(1/T^{1.27})$.
                                </p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p>
                            All of these sequences involve the <i>silver ratio</i> \(\rho = 1+\sqrt{2}\) and roughly have the form \(\eta_k = \rho^{\nu(k)}\), where \(\nu(i)\) is the largest power of 2 dividing \(i\).
                        </p>
                        <img src="step_sizes.png" height="500px"></img>
                    </section>
                    <section data-auto-animate>
                        <h4>Performance estimation problem</h4>
                        <p>
                            We want to prove a specific bound of the following form:
                        </p>
                        <div style="background: grey; color: white; padding: 20px 20px 20px 20px; margin-right:10px">
                            <h5 style="color: white;">Theorem (Grimmer, S., Wang)</h5>
                            <p>
                            If $x_i$ is the $i^{th}$ iterate of this procedure, and $x_*$ is the minimizer, then
                            </p>
                            <div id="ineq">
                            \[
                                f(x_{2^k}) - f_{*} \le r_k \|x_0 - x_*\|^2 - Q,
                            \]
                            </div>
                            <p>
                            where $r_k \le (1+\sqrt{2})^{-k}$, and $y \in \R^n$, and $Q$ is a positive semidefinite quadratic form in the problem variables.
                            </p>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4 id="h4123">Performance estimation problem</h4>
                        <div id="thm123" style="background: grey; color: white; padding: 20px 20px 20px 20px; margin-right:10px">
                            <div id="ineq">
                            \[
                                f(x_{2^k}) - f_{*} \le r_k \|x_0 - x_*\|^2 - Q,
                            \]
                            </div>
                        </div>
                        <p class="fragment fade-in">
                            Smooth convex functions satisfy the following inequalities for any $x, y \in \R^n$.
                            \[
                                f(x) - f(y) \ge \langle \nabla f(y), x - y\rangle + \frac{1}{2L} \|\nabla f(x) - \nabla f(y)\|^2.
                            \]
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4 id="h4123">Performance estimation problem</h4>
                        <div  id="thm123" style="background: grey; color: white; padding: 20px 20px 20px 20px; margin-right:10px">
                            <div id="ineq">
                            \[
                                f(x_{2^k}) - f_{*} \le r_k \|x_0 - x_*\|^2 - Q,
                            \]
                            </div>
                        </div>
                        <p class="fragment fade-in">
                            If we denote by $g_i = \nabla f(x_i)$, then this inequality can be written
                            \[
                                f(x_j) - f(x_i) \ge q_{ij}(g_1, \dots, g_n),
                            \]
                            where $q_{ij}$ is a quadratic form.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4>Performance estimation problem</h4>
                        <div style="background: grey; color: white; padding: 20px 20px 20px 20px; margin-right:10px">
                            <div id="ineq">
                            \[
                                f(x_{2^k}) - f_{*} \le r_k \|x_0 - x_*\|^2 - Q,
                            \]
                            </div>
                        </div>
                        <p>
                            If we denote by $g_i = \nabla f(x_i)$, then this inequality can be written
                            \[
                                f(x_j) - f(x_i) \ge q_{ij}(g_1, \dots, g_n),
                            \]
                            where $q_{ij}$ is a quadratic form.
                        </p>
                        <p class="fragment fade-in">
                            By taking nonnegative combinations of these inequalities, we can get new inequalities.
                            This is called the performance estimation problem (Drori and Teboulle).
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4>Performance estimation problem</h4>
                        <div style="background: grey; color: white; padding: 20px 20px 20px 20px; margin-right:10px">
                            <div id="ineq">
                            \[
                                f(x_{2^k}) - f_{*} \le r_k \|x_0 - x_*\|^2 - Q,
                            \]
                            </div>
                        </div>
                        <p>
                            We found a specific nonnegative combination of these defining inequalities to prove this result.
                        </p>
                        <img src="ineq1.png"></img>
                    </section>
                    <section data-auto-animate>
                        <h4>Performance estimation problem</h4>
                        <div style="background: grey; color: white; padding: 20px 20px 20px 20px; margin-right:10px">
                            <div id="ineq">
                            \[
                                f(x_{2^k}) - f_{*} \le r_k \|x_0 - x_*\|^2 - Q,
                            \]
                            </div>
                        </div>
                        <p>
                            We found a specific nonnegative combination of these defining inequalities to prove this result.
                        </p>
                        <img src="weights.png"></img>
                    </section>
                    <section>
                        <h4> Further Directions </h4>

                        <ol>
                            <li class="fragment fade-in"> Lower bounds? </li>
                            <li class="fragment fade-in"> Have step size depend on the previous gradients (but still only move in gradient directions)? </li>
                            <li class="fragment fade-in"> Can this be adapted to other settings? Stochastic or manifold settings? </li>
                        </ol>
                    </section>
                    <section>
                        <h4>References</h4>
                        <ol>
                            <li> Grimmer, Benjamin, Kevin Shu, and Alex L. Wang. "Accelerated gradient descent via long steps." arXiv preprint arXiv:2309.09961 (2023). </li>
                            <li> Altschuler, Jason M., and Pablo A. Parrilo. "Acceleration by stepsize hedging II: Silver stepsize schedule for smooth convex optimization." arXiv preprint arXiv:2309.16530 (2023).</li>
                        </ol>
                    </section>
                </section>

                
				<section>
                    <section>
                        <h2>Hidden Convexity and Homotopy Theory </h2>
                    </section>
                    <section>
                        <h3> Based on joint work with Akshay Ramachandran and Alex Wang </h3>
                    </section>
				</section>

                <section>
                    <section data-background-color="black" data-auto-animate>
                        <h4>A Simple Problem</h4>
                        <p style="color: white" data-id="problemDesc">
                            Imagine you're an astronaut who is floating in space, and you can see the earth below you.
                        </p>
                        <img src="earth.svg" width="500px" data-id="earth"></img>
                    </section>
                    <section data-background-color="black" data-auto-animate>
                        <h4>A Simple Problem</h4>
                        <p style="color: white" data-id="problemDesc">
                            You can also see some landmarks. You want to determine how you are oriented in space based on a globe.
                        </p>
                        <img src="earth2.svg" width="500px" data-id="earth"></img>
                    </section>
                    <section data-auto-animate>
                        <h4> Wahba's Problem</h4>
                        <p>
                            Given $\{u_i\}_{i=1}^n, \{v_i\}_{i=1}^n \in \R^3$, find
                            \[
                                \min_{U \in \text{SO}(3)} \sum_{i=1}^n \|U v_i - u_i\|^2,
                            \]
                            where $\text{SO}(3)$ is the set of $3 \times 3$ rotation matrices.
                        </p>
                        <p class="fragment fade-in">
                        We think of the $u_i$ as being the reference points and the $v_i$ as being the landmarks.
                        </p>
                    </section>
                            
                    <section data-auto-animate>
                        <h4> Wahba's Problem (Reformulated)</h4>
                        <p>
                            \[
                                \min_{U \in \text{SO}(3)} \langle V, U \rangle,
                            \]
                            where \(V = -\sum_{i} u_i v_i^{\intercal}.\)
                            
                            This can be solved using SVDs.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4 data-id="constrainedwahba"> Constrained Wahba's Problem</h4>
                        <p>
                            Given \(A, B \in \R^{n \times n}\), find
                        </p>
                        <p>
                            \[
                                \min_{U \in \text{SO}(n)} \langle A, U \rangle
                            \]
                            \[
                                \text{s.t. } \langle B, U \rangle \ge c
                            \]
                        </p>
                        <p class="fragment fade-in">
                            A constraint might appear if the astronaut already has some initial estimate of their orientation.
                            The constraint reduces symmetry and means that <b>a direct solution using SVDs is not possible</b>.
                        </p>
                    </section>
                    <section data-auto-animate>
                        <h4 data-id="constrainedwahba"> Constrained Wahba's Problem (Reformulated) </h4>
                        <p>
                            Given \(A, B \in \R^{n \times n}\), find
                        </p>
                        <p>
                            \[
                            \begin{align}
                                \min &\qquad x\\
                                \text{s.t. }&\qquad y \ge c\\
                                &\qquad(x,y) \in \{\langle A, U\rangle, \langle B, U\rangle : U \in \text{SO}(n)\}.
                            \end{align}
                            \]
                        </p>
                        <div style="background: grey; color: white; padding: 20px 20px 20px 20px" class="fragment fade-in">
                            <h5 style="color: white;">Theorem <!--CITE--></h5>
                            <p >
                                For any \(A, B \in \mathbb{R}^{n\times n}\), \(\{\langle A, U\rangle, \langle B, U\rangle : U \in \text{SO}(n)\}\) is convex.
                                <sup>1,2</sup>
                            </p>
                        </div>
                        <p class="fragment fade-in">
                            This leads to an \(\epsilon\) approximation algorithm for the constrained Wahba's problem in \(O(n^3 \log(\frac{1}{\epsilon}))\) time.
                        </p>
                    </section>
                    <section>
                        <h4> Some projections </h4>
                        <div style="display:flex">
                            <div style="flex:1">
                                <img src="projection.png" width="200px" height="200px"></img>
                            </div>
                            <div style="flex:1">
                                <img src="proj2.png" width="200px" height="200px"></img>
                            </div>
                            <div style="flex:1">
                                <img src="proj3.png" width="200px" height="200px"></img>
                            </div>
                        </div>
                    </section>
                    <section data-auto-animate>
                        <h4> Hidden convexity in other contexts</h4>
                        <ul>
                            <li class="fragment fade-in" >
                                We've \(T : \R^{n\times n}\rightarrow \R^2\), \(T(\text{SO}(n))\) is convex. <b>New</b>
                            </li>
                            <li class="fragment fade-in" >
                                If \(T  : \R^{n \times n} \rightarrow \R^n \) projects a matrix onto its diagonal entries, then \(T(\text{SO}(n))\) is convex. <i>(Horn's theorem)</i>
                            </li>
                            <li class="fragment fade-in" >
                                <p>If $Q_1, Q_2 : \R^{n} \rightarrow \R$ are quadratic forms, then $\{(Q_1(x), Q_2(x)) : \|x\|_2 = 1\}$ is convex <b>(Brickman's theorem)</b></p>
                            </li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h4> Hidden convexity in other contexts</h4>
                        <ul>
                            <li class="fragment fade-in" >
                                If $A_1, \dots, A_k \in \R^{n\times n}_{sym}$ with the property that no nonzero matrix in their span has a repeated maximum eigenvalue, then 
                                \[
                                    \{(x^{\intercal}A_1x, \dots, x^{\intercal}A_kx) : \|x\|_2 = 1\}
                                \]
                                is convex. (Gutkin, Jonckheere, and Karow)
                            </li>
                            <li class="fragment fade-in" >
                                Fix $\lambda \in \R^n$. If $M_{\lambda}$ is the set of symmetric matrix with eigenvalues $\lambda$, and $T :\R^{n \times n}_{sym} \rightarrow \R^2$ is a linear map, then 
                                $T(M_{\lambda})$ is convex. (Au-Yeung and Tsing)
                            </li>
                        </ul>
                    </section>
                    <section data-auto-animate>
                        <h4> Origins of hidden convexity </h4>
                        In general, we might want to know, for a topological space $X$ and a continuous function $f : X \rightarrow \R^k$, under what conditions might $f(X)$ be convex?

                        We will give a criterion that implies the previous theorems (except for Horn's theorem).
                    </section>
                    <section data-auto-animate>
                        <h4> Origins of hidden convexity </h4>
                        For a convex set $Y \subseteq \R^n$, its <b>support function</b> at a vector $v \in S^{n-1}$ is
                        \[
                            \sigma(v) = \text{max} \{\langle v, y \rangle : y \in Y\}.
                        \]
                        We will consider <i>smooth</i> convex sets, where for each $v$, there is a unique maximizer of $\langle v, y\rangle$ in $Y$. In this case, we can consider the map 
                        \[
                            h : S^{n-1} \rightarrow Y,
                        \]
                        which sends $v$ to this unique maximizer.
                    </section>
                    <section data-auto-animate>
                        <h4> Origins of hidden convexity </h4>
                        <p>Let $Y$ be the convex hull of $f(X)$, where $X$ is a topological space.</p>
                        <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                            width="329.082px" height="322.483px" viewBox="0 0 329.082 322.483" enable-background="new 0 0 329.082 322.483"
                            xml:space="preserve">
                        <path fill="#7FC45C" stroke="#225A2D" stroke-miterlimit="10" d="M557.025-82.496"/>
                        <g id="arrowH" class="fragment fade-in">
                            <g>
                                <path d="M38.401,223.699c0,1.891,0.438,3.301,1.313,4.23s2.242,1.395,4.102,1.395c1.438,0,2.672-0.266,3.703-0.797
                                    s1.813-1.285,2.344-2.262s0.797-2.137,0.797-3.48c0-0.953-0.168-1.793-0.504-2.52s-0.844-1.422-1.523-2.086
                                    s-1.66-1.426-2.941-2.285c-1.156-0.781-2.117-1.551-2.883-2.309s-1.375-1.605-1.828-2.543s-0.68-1.984-0.68-3.141
                                    c0-1.844,0.445-3.469,1.336-4.875s2.148-2.492,3.773-3.258s3.5-1.148,5.625-1.148c1.438,0,2.77,0.098,3.996,0.293
                                    s2.582,0.527,4.066,0.996l-1.313,6.023h-2.25c-0.063-1.328-0.266-2.371-0.609-3.129s-0.836-1.301-1.477-1.629
                                    s-1.492-0.492-2.555-0.492c-1.297,0-2.438,0.266-3.422,0.797s-1.734,1.254-2.25,2.168s-0.773,1.934-0.773,3.059
                                    c0,0.813,0.141,1.539,0.422,2.18s0.73,1.27,1.348,1.887s1.52,1.332,2.707,2.145c1.531,1.063,2.699,2.012,3.504,2.848
                                    s1.398,1.723,1.781,2.66s0.574,2.023,0.574,3.258c0,1.891-0.457,3.574-1.371,5.051s-2.203,2.617-3.867,3.422
                                    s-3.559,1.207-5.684,1.207c-1.453,0-2.996-0.109-4.629-0.328s-3.129-0.516-4.488-0.891l1.406-6.445H38.401z"/>
                            </g>
                            <g>
                                <path d="M264.421,198.996l-0.258,1.148c-0.656,0.063-1.156,0.254-1.5,0.574s-0.516,0.816-0.516,1.488
                                    c0,0.953,0.219,2.203,0.656,3.75l2.484,8.836l5.766-7.711c0.859-1.141,1.48-2.035,1.863-2.684s0.652-1.223,0.809-1.723
                                    c0.125-0.375,0.188-0.734,0.188-1.078c0-0.438-0.148-0.773-0.445-1.008s-0.727-0.383-1.289-0.445l0.258-1.148h8.789l-0.258,1.148
                                    c-0.375,0.078-0.699,0.191-0.973,0.34s-0.586,0.387-0.938,0.715s-0.828,0.855-1.43,1.582s-1.371,1.699-2.309,2.918l-9.117,11.836
                                    l-1.453,6.633c-0.141,0.625-0.238,1.113-0.293,1.465s-0.102,0.727-0.141,1.125s-0.059,0.754-0.059,1.066
                                    c0,0.484,0.078,0.863,0.234,1.137s0.391,0.473,0.703,0.598s0.852,0.219,1.617,0.281l-0.258,1.148h-9.867l0.258-1.148
                                    c0.625-0.094,1.086-0.219,1.383-0.375s0.559-0.398,0.785-0.727s0.453-0.844,0.68-1.547s0.496-1.766,0.809-3.188l1.383-6.469
                                    l-3.398-11.836c-0.516-1.813-0.922-3.039-1.219-3.68s-0.613-1.094-0.949-1.359s-0.777-0.438-1.324-0.516l0.258-1.148H264.421z"/>
                            </g>
                            <g>
                                <path d="M69.763,191.908h0.375c0.242,0,0.504-0.047,0.785-0.141s0.582-0.24,0.902-0.439s0.75-0.514,1.289-0.943
                                    c0.406-0.328,0.707-0.607,0.902-0.838s0.328-0.432,0.398-0.604s0.105-0.336,0.105-0.492c0-0.383-0.266-0.598-0.797-0.645
                                    l0.129-0.574h3.797l0.223,0.574c-0.648,0.586-1.336,1.16-2.063,1.723l-2.965,2.309l1.758,4.441c0.156,0.398,0.316,0.705,0.48,0.92
                                    s0.354,0.373,0.568,0.475s0.49,0.164,0.826,0.188l-0.129,0.574H73.22l-1.711-4.816c-0.109-0.305-0.203-0.512-0.281-0.621
                                    s-0.168-0.188-0.27-0.234s-0.266-0.07-0.492-0.07h-0.867l-1.219,5.742h-2.027l3.012-13.395c0.086-0.391,0.146-0.674,0.182-0.85
                                    s0.061-0.338,0.076-0.486s0.023-0.281,0.023-0.398c0-0.328-0.1-0.559-0.299-0.691s-0.557-0.207-1.072-0.223l0.141-0.609
                                    l2.965-0.105h0.68L69.763,191.908z"/>
                            </g>
                            <g>
                                <g>
                                    <polygon points="104.451,220.521 205.666,220.521 205.666,232.09 228.451,218.936 205.666,205.781 205.666,217.349 
                                        104.451,217.349 			"/>
                                </g>
                            </g>
                            <g>
                                <path d="M159.884,255.936h-3.68l1.477-5.953c0.109-0.43,0.205-0.865,0.287-1.307s0.123-0.842,0.123-1.201
                                    c0-0.563-0.094-0.963-0.281-1.201s-0.504-0.357-0.949-0.357c-0.32,0-0.674,0.133-1.061,0.398s-0.795,0.67-1.225,1.213
                                    s-0.771,1.063-1.025,1.559s-0.455,1.088-0.604,1.775l-1.066,5.074h-2.027l3.012-13.395c0.086-0.391,0.146-0.674,0.182-0.85
                                    s0.061-0.338,0.076-0.486s0.023-0.281,0.023-0.398c0-0.328-0.1-0.559-0.299-0.691s-0.557-0.207-1.072-0.223l0.141-0.609
                                    l2.965-0.105h0.68l-2.016,8.121l0.141,0.047c0.734-0.977,1.434-1.688,2.098-2.133s1.34-0.668,2.027-0.668
                                    c0.758,0,1.332,0.209,1.723,0.627s0.586,1.037,0.586,1.857c0,0.5-0.078,1.102-0.234,1.805l-0.984,4.172
                                    c-0.078,0.336-0.133,0.605-0.164,0.809s-0.047,0.379-0.047,0.527c0,0.242,0.041,0.43,0.123,0.563s0.207,0.232,0.375,0.299
                                    s0.436,0.119,0.803,0.158L159.884,255.936z"/>
                            </g>
                        </g>
                        <g id="arrowF" class="fragment fade-in">
                            <g>
                                <path d="M265.664,8.996l-0.258,1.148c-0.672,0.109-1.207,0.34-1.605,0.691s-0.598,0.863-0.598,1.535c0,1,0.258,2.258,0.773,3.773
                                    l1.945,5.672l5.016-6.352c0.234-0.297,0.465-0.605,0.691-0.926s0.43-0.633,0.609-0.938s0.313-0.566,0.398-0.785
                                    s0.148-0.414,0.188-0.586s0.059-0.336,0.059-0.492c0-0.422-0.125-0.781-0.375-1.078s-0.68-0.469-1.289-0.516l0.258-1.148h8.742
                                    l-0.258,1.148c-0.469,0.141-0.91,0.367-1.324,0.68s-0.906,0.781-1.477,1.406s-1.629,1.859-3.176,3.703L267,24.277l3.422,9.516
                                    c0.516,1.438,0.93,2.523,1.242,3.258c0.344,0.781,0.641,1.352,0.891,1.711c0.203,0.266,0.414,0.477,0.633,0.633
                                    c0.219,0.141,0.586,0.289,1.102,0.445l-0.234,1.148h-9.469l0.234-1.148c0.719-0.109,1.27-0.355,1.652-0.738
                                    s0.574-0.879,0.574-1.488c0-0.922-0.188-1.945-0.563-3.07l-2.438-7.148l-5.344,6.68c-0.797,1-1.375,1.816-1.734,2.449
                                    s-0.539,1.176-0.539,1.629c0,0.375,0.051,0.672,0.152,0.891s0.273,0.395,0.516,0.527s0.574,0.223,0.996,0.27l-0.258,1.148h-8.719
                                    l0.234-1.148c0.313-0.109,0.59-0.23,0.832-0.363s0.508-0.32,0.797-0.563s0.68-0.637,1.172-1.184s1.051-1.188,1.676-1.922
                                    l9.141-10.875l-3.469-9.633c-0.344-0.953-0.648-1.75-0.914-2.391s-0.527-1.148-0.785-1.523s-0.527-0.656-0.809-0.844
                                    s-0.633-0.32-1.055-0.398l0.258-1.148H265.664z"/>
                            </g>
                            <g>
                                <g>
                                    <polygon points="264.365,53.936 264.365,155.15 252.796,155.15 265.951,177.936 279.105,155.15 267.538,155.15 267.538,53.936 
                                                    "/>
                                </g>
                            </g>
                            <g>
                                <path d="M286.427,119.635c-0.266,1.25-0.631,2.242-1.096,2.977s-1.029,1.262-1.693,1.582s-1.469,0.48-2.414,0.48
                                    c-0.344,0-0.652-0.039-0.926-0.117l0.27-1.031c0.188,0.063,0.461,0.094,0.82,0.094c0.414,0,0.752-0.055,1.014-0.164
                                    s0.504-0.287,0.727-0.533s0.434-0.605,0.633-1.078s0.389-1.096,0.568-1.869l2.379-10.559h-1.418l0.141-0.598
                                    c0.383-0.039,0.652-0.092,0.809-0.158s0.283-0.156,0.381-0.27s0.186-0.273,0.264-0.48s0.191-0.564,0.34-1.072
                                    c0.414-1.406,1.053-2.463,1.916-3.17s1.963-1.061,3.299-1.061c0.766,0,1.426,0.063,1.98,0.188l-0.457,1.992h-1.031
                                    c-0.109-0.445-0.252-0.766-0.428-0.961s-0.436-0.293-0.779-0.293c-0.445,0-0.824,0.121-1.137,0.363s-0.58,0.607-0.803,1.096
                                    s-0.428,1.143-0.615,1.963l-0.293,1.277h2.684l-0.258,1.184h-2.695L286.427,119.635z"/>
                            </g>
                            <g>
                            </g>
                        </g>
                        <g id="arrowPhi" class="fragment fade-in">
                            <g>
                                <g>
                                    <polygon points="101.561,172.568 208.285,68.014 216.38,76.276 223.451,50.936 197.97,57.484 206.064,65.747 99.34,170.302 			
                                        "/>
                                </g>
                            </g>
                            <g>
                                <path d="M146.532,112.855l2.042,2.085l-1.071,1.05l-2.05-2.093c-1.065,0.923-2.038,1.417-2.92,1.482s-1.694-0.281-2.438-1.041
                                    c-0.53-0.541-0.864-1.25-1.001-2.127c-0.138-0.877-0.063-1.878,0.226-3.002l0.885,0.134c-0.097,0.85-0.063,1.571,0.102,2.164
                                    s0.464,1.109,0.896,1.55c0.596,0.608,1.196,0.931,1.801,0.968s1.235-0.196,1.893-0.697l-2.19-2.235
                                    c-0.568-0.58-0.942-1.151-1.122-1.715c-0.18-0.563-0.203-1.116-0.07-1.662c0.132-0.545,0.417-1.031,0.852-1.458
                                    c0.385-0.377,0.82-0.631,1.305-0.761c0.485-0.131,1-0.102,1.545,0.086s1.083,0.553,1.613,1.094
                                    C148.713,108.603,148.615,110.662,146.532,112.855z M145.991,112.304c0.665-0.762,0.961-1.506,0.889-2.233
                                    c-0.073-0.727-0.5-1.489-1.282-2.287c-0.602-0.614-1.136-0.989-1.604-1.127s-0.861-0.051-1.179,0.261
                                    c-0.301,0.296-0.437,0.612-0.408,0.949c0.03,0.338,0.18,0.696,0.449,1.078c0.27,0.381,0.683,0.856,1.241,1.425L145.991,112.304z"
                                    />
                            </g>
                        </g>
                        </svg>
                        <p class="fragment fade-in">If there is a continuous such lift $\phi$, we say $f$ is continuously maximized.</p>
                    </section>
                    <section>
                        <h4> Origins of hidden convexity </h4>
                        <p>
                            The map $\phi$ defines an element of the homotopy group $\pi_{n-1}(X)$.
                        </p>
                        <p>
                            If $\pi_{n-1}(X) = 0^*$, then continuous maximization implies convexity.
                        </p>
                        <video controls><source src="QuadraticSphereMap2.mp4" type="video/mp4"></video>
                    </section>
                    <section>
                        <h4> Hidden convexity of $\SO(n)$ </h4>
                        \[
                            T : \SO(n) \rightarrow \R^2
                        \]
                        <ul>
                        <li class="fragment fade-in"><p>
                            Assume $T$ is generic, then <i>Von Neumann-Wigner</i> implies that the image $T^{\intercal}$ has no matrices with a repeated singular value.
                        </p></li>
                        <li class="fragment fade-in"><p>
                            This implies that the singular vectors of matrices in the image of $T^{\intercal}$ are <i>continuous</i> except at the origin.
                            In our context, this implies that $T$ is continuously maximized.
                        </p></li>
                        <li class="fragment fade-in"><p>
                            $\pi_1(\SO(n)) = \Z / 2 \Z$. This is enough for $T(\SO(n))$ to be convex.
                        </p></li>

                        </ul>
                    </section>
                    <section>
                        <h4> Noncrossing subspaces </h4>
                        <div style="background: grey; color: white; padding: 20px 20px 20px 20px" class="fragment fade-in">
                            <h5 style="color: white;">Theorem <!--CITE--></h5>
                            <p >
                            A linear map \(T : \R^{n \times n} \rightarrow \R^k \) is <i> singularly noncrossing</i> if every nonzero matrix in the image of \(T^{\intercal}\) has nondegenerate singular values.
                            </p>
                            <p  class="fragment fade-in">
                            If \(T\) is singularly noncrossing and \(\text{SO}(n)\) satisfies certain topological properties concerning its homotopy groups, then \(T(\text{SO}(n))\) is convex.
                            </p>
                        </div>
                        <p class="fragment fade-in"> Noncrossing maps are closely related to representations of Clifford algebras.</p>
                        <p class="fragment fade-in"> Von Neumann-Wigner says that generic 2 dimensional subspaces are noncrossing</p>
                    </section>
                </section>
                <section>
                    <h4>References</h4>
                    <ol>
                        <li> Ramachandran, Akshay, Kevin Shu, and Alex L. Wang. "Hidden convexity, optimization, and algorithms on rotation matrices." arXiv preprint arXiv:2304.08596 (2023). </li>
                        <li> Li, Chi-Kwong, and Tin-Yau Tam. "Numerical ranges arising from simple Lie algebras." Canadian Journal of Mathematics 52.1 (2000): 141-171. </li>
                    </ol>
                </section>

                <section>
                    <section>
                    <h3>Hyperbolic Polynomials and Eigenvalue Problems</h3>
                    </section>
                    <section>
                        <h3> Based on joint work with various people </h3>
                    </section>
                </section>
                <section>
                    <section>
                    <p><b>Fact: </b>If $X$ is a PSD matrix, then every principal submatrix of $X$ is PSD.</p>
                    <div style="background: grey; color: white; padding: 20px 20px 20px 20px" class="fragment fade-in">
                        <h5 style="color: white;">Question</h5>
                        <p >
                            If $X$ has the property that all of the $k \times k$ principal submatrices of $X$ are PSD, how far can $X$ be from being PSD?
                        </p>
                    </div>
                    </section>
                    <section>
                        <p>Formally, we let $S^{n,k}$ be the set of $n\times n$ matrices where all of the $k \times k$ principal submatrices are PSD.</p>
                        <p>We want to find</p>
                        \[
                            \min \{\lambda_{min}(X) : X \in S^{n,k},\; \tr(X)=1\}.
                        \]
                    </section>
                    <section>
                    </section>
                    <section>
                        <h4>Applications of $S^{n,k}$</h4>
                        <p>
                            The dual to \(S^{n,k}\) is the <i>factor width \(k\)</i> cone, which can be used to describe sparse quadratic programs like sparse regression and sparse PCA.
                            \[
                                (S^{n,k})^* = \{X \text{ symmetric} : X = \sum v_i v_i^{\intercal}\text{ where }\|v_i\|_0 \le k\}
                            \]
                        </p>
                        <p>
                            Sparse regression and sparse PCA can be expressed as conic optimization problems over \(S^{n,k}\).
                        </p>
                        <p>
                            That is, they are of the form
                            <div>
                                \[\text{Minimize }\langle A, X\rangle\]
                                \[\text{such that }\langle B, X\rangle = 1\]
                                \[\qquad X \in (S^{n,k})^*\]
                            </div>
                        </p>
                    </section>
                    <section>
                        <h4>Characteristic Polynomials in $S^{n,k}$</h4>
                        <p><b>Observation: </b> If $X \in S^{n,k}$, then for any $\ell \le k$,
                        \[
                            c^{n,\ell}(X) = \sum_{\substack{S \subseteq [n] \\ |S| = \ell}} \det(X|_S) \ge 0.
                        \]
                        </p>
                        <p class="fragment fade-in">
                            The characteristic polynomial of $X$ is 
                            \[
                                \det(X - tI) = \sum_{\ell=0}^n (-1)^{\ell} c^{n,n-\ell}(X) t^{\ell}.
                            \]
                        </p>
                    </section>
                    <section>
                        <h4>Characteristic Polynomials in $S^{n,k}$</h4>
                        <p>
                            This means that if $X \in S^{n,k}$, then the first $k$ coefficients of the characteristic polynomial of $X$ alternate in sign.
                            The set of symmetric matrices with this property is <b>convex!</b>
                        </p>
                        <p class="fragment fade-in">
                            This set is called the <i>hyperbolicity cone</i> of the polynomial $c^{n,k}$.
                        </p>
                        <div style="background: grey; color: white; padding: 20px 20px 20px 20px" class="fragment fade-in">
                            <h5 style="color: white;">Theorem<sup>1</sup></h5>
                            <p >
                                For any \(X \in S^{n,k}, \lambda_{\min}(X) \ge \frac{k-n}{n(k-1)}\text{tr}(X)\), and this inequality is met with equality for every \(n\) and \(k\).
                            </p>
                        </div>
                    </section>
                </section>
                <section>
                    <h4>Hyperbolic Polynomials and Eigenvalue Problems</h4>
                    <section>
                        <p>
                            A polynomial \(f(x)\) is <i>hyperbolic</i> with respect to \(v \in \mathbb{R}^n\) if for every \(x \in \mathbb{R}^n\),
                            \(
                                g_x(t) = f(x + tv)
                            \)
                            has only real roots.
                        </p>
                        <p class="fragment fade-in">
                            An important example comes from the determinant of a symmetric matrix, which is hyperbolic with respect to the identity matrix by the spectral theorem.
                        </p>
                    </section>
                    <section>
                        <p>
                            Hyperbolic polynomials have associated hyperbolicity cones:
                            \[
                            \Lambda(f, v) = \{x \in \mathbb{R}^n : f(x+tv) > 0 \text{ for }t > 0\}.
                            \]
                            These are convex cones associated to the polynomial that generalize the positive semidefinite cone.
                        </p>
                    </section>
                    <section>
                        <p>
                            The \(k^{th}\) Renegar derivative of the PSD cone is given by \(\Lambda(c_{n-k}, I),\)

                            where \(c_k\) is the chararacteristic coefficient defined by \[c_{n-k}(X) = D_I^{k} \det(X).\]
                        </p>
                        <p class="fragment fade-in">
                            These are nice basis invariant convex cones of matrices, which are larger than the PSD cone. 
                            When \(k = n -2\), this is the second order cone.
                        </p>
                    </section>
                    <section>
                        <p>
                            \(S^{n,k}\) is related to the Renegar derivatives because \[S^{n,k} \subseteq \Lambda(c_{n-k}, I).\]
                        </p>
                        <p class="fragment fade-in">
                            This leads to some natural theorems about the matrices in \(S^{n,k}\):
                        </p>
                        <div style="background: grey; color: white; padding: 20px 20px 20px 20px" class="fragment fade-in">
                            <h5 style="color: white;">Theorem<sup>1</sup></h5>
                            <p >
                                For any \(X \in S^{n,k}, \lambda_{\min}(X) \ge \frac{k-n}{n(k-1)}\text{tr}(X)\), and this inequality is met with equality for every \(n\) and \(k\).
                            </p>
                        </div>
                    </section>
                    <section>
                        <p>I have explored a number of other questions along these lines:</p>
                        <ul>
                            <li> We considered related eigenvalue minimization problems over sets of matrices where some combinatorially specified set of submatrices are PSD.<sup>2</sup></li>
                            <li> We considered generalizations of the Renegar derivatives of the PSD cone, and showed how many known results from linear algebra can be generalized to these new settings. <sup>3</sup></li>
                            <li> We considered symmetric hyperbolic polynomials, and characterized what these look like in the cubic case, as well as giving some spectrahedral descriptions of their hyperbolicity cones. <sup>4</sup></li>
                        </ul>
                    </section>
                    <section>
                        <h4>References</h4>
                        <ol>
                            <li> Blekherman, Grigoriy, et al. "Hyperbolic relaxation of k-locally positive semidefinite matrices." SIAM Journal on Optimization 32.2 (2022): 470-490. </li>
                            <li> Blekherman, Grigoriy, and Kevin Shu. "Sums of squares and sparse semidefinite programming." SIAM Journal on Applied Algebra and Geometry 5.4 (2021): 651-674. </li>
                            <li> Blekherman, Grigoriy, et al. "Linear principal minor polynomials: hyperbolic determinantal inequalities and spectral containment." International Mathematics Research Notices 2023.24 (2023): 21346-21380. </li>
                            <li> Blekherman, Grigoriy, Julia Lindberg, and Kevin Shu. "Symmetric Hyperbolic Polynomials." arXiv preprint arXiv:2308.09653 (2023). </li>
                        </ol>
                    </section>
                </section>



			</div>
		</div>

		<script src="reveal.js"></script>
		<script src="math.js"></script>
		<script>
			Reveal.initialize({
				history: true,
				transition: 'linear',

				mathjax2: {
					config: 'TeX-AMS_HTML-full',
					TeX: {
						Macros: {
							R: '\\mathbb{R}',
							Z: '\\mathbb{Z}',
							set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ],
							SO: '\\text{SO}',
							tr: '\\text{tr}',
						}
					}
				},

				// There are three typesetters available
				// RevealMath.MathJax2 (default)
				// RevealMath.MathJax3
				// RevealMath.KaTeX
				//
				// More info at https://revealjs.com/math/
				plugins: [ RevealMath.MathJax2 ]
			});
		</script>

	</body>
</html>
